# ğŸŒ¸ K-Nearest Neighbors (KNN) Classification â€“ Iris Dataset

## ğŸ“Œ Project Overview

This project implements **K-Nearest Neighbors (KNN)** classification using the Iris dataset.

The objective is to understand instance-based learning, Euclidean distance, K selection, normalization, and decision boundaries.

This project is completed as part of **AI & ML Internship â€“ Task 6**.

---

## ğŸ¯ Objective

- Implement KNN classifier
- Normalize features
- Experiment with different K values
- Evaluate model using accuracy and confusion matrix
- Visualize decision boundaries

---

## ğŸ›  Tools Used

- Python  
- Pandas  
- NumPy  
- Matplotlib  
- Scikit-learn  

---

## ğŸ“‚ Dataset

Iris Dataset (loaded from sklearn)

Target Classes:

- 0 â†’ Setosa  
- 1 â†’ Versicolor  
- 2 â†’ Virginica  

---

## âš™ï¸ Steps Performed

### 1. Load Dataset

Used sklearn built-in iris dataset.

---

### 2. Train-Test Split

80% training  
20% testing  

---

### 3. Feature Normalization

Applied StandardScaler since KNN is distance-based.

---

### 4. Experiment with Different K Values

Tested K from 1 to 20 and plotted accuracy.

---

### 5. Model Evaluation

Evaluated using:

- Accuracy  
- Confusion Matrix  

---

### 6. Decision Boundary Visualization

Plotted 2D decision boundary using first two features.

---

## ğŸ“Š Results

Outputs include:

- Accuracy Score  
- Confusion Matrix  
- K vs Accuracy Plot  
- Decision Boundary Plot  

---

## ğŸ§  Learning Outcomes

- Instance-based learning  
- Role of distance metrics  
- Importance of normalization  
- K selection strategy  
- Visualization of classification boundaries  

---

## ğŸ‘©â€ğŸ’» Author

Ushasree Mundra

---

## â­ Conclusion

This project demonstrates how KNN works using Euclidean distance and how selecting the correct value of K improves classification performance.
